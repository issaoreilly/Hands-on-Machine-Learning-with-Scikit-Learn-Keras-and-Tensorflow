{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350ec24b-2a16-4402-b290-13058595bb4a",
   "metadata": {},
   "source": [
    "# 1. types of ML systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71adfbad-28ef-45db-af10-c3da9de8cb8c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "- Supervised vs unsupervised learning\n",
    "- Batch or online:\n",
    "    - Whenever the data and the model can fit into the main memory, then we can apply batch learning.\n",
    "- Instance-based vs. model-based\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa16122-8898-45ef-8824-58d1c1cdced9",
   "metadata": {},
   "source": [
    "# 2. Main Challanges of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaea89b-cc2d-4191-b440-6cce0f11dda3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "Since the main task is to select a learning algorithm and train it on some data, the two things that can go wrong are **\"bad algorithms\"** and **\"bad data\"**. Let's start with examples of bad data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebdf60-e8d9-44e3-8b7a-865a2192c805",
   "metadata": {},
   "source": [
    "## 2.1. Bad Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61da66-46cb-4733-80ac-3770604d9be4",
   "metadata": {},
   "source": [
    "### 2.1.1 Insufficient Quantity of Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221395c-8589-4e06-9a8c-53697a482b42",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "Machine learning requires a lot of data for most algorithms to work properly. Even for very simple algorithms you typically need thousands of examples, and for complex problems such as images or speech recognition you may need millions of examples (unless you can reuse parts of an existing model).\n",
    "    \n",
    "**The Unreasonable Effectiveness of Data**: It was proven that very different machine learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natual language disambiguation once they were given enough data. Hence, data matters more than algorithms for complex problems. However, since small and medium-sized datasets are still very common, it is not always easy or cheap to get extra training data - so we can't abandon algorithms just yet.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c138a2-5ade-4643-83bb-7b66a1284ee5",
   "metadata": {},
   "source": [
    "### 2.1.2 Nonrepresentative Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d858d6-5974-48d7-8bc6-fa00d9fce40c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "In order to generalize well, it is critical that your training data be representative of new cases you want to generalize to. This is true whether you use instance-based or model-based learning.\n",
    "    \n",
    "By using a nonrepresentative training set, we trained a model that is unlikely to make accurate predictions on new dataset. Hence, it is crucial to use a training set that is representative of the cases you want to generalize to. This is often harder than it sounds: <br>\n",
    "- ***Sampling noise***: This is the case if the sample is too small; i.e., nonrepresentative data as a result of chance.\n",
    "- ***Sampling bias***: A very large dataset can be nonrepresentative if the sampling method is flawed.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a66e3-5f08-4413-9b39-4fb1dcbbaa4a",
   "metadata": {},
   "source": [
    "### 2.1.3 Poor Quality Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e550ad-137a-4743-b865-47ff1705843a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientist spend a significant part of their time doing just that. The following is just an example of when you'd want to clean up training data:\n",
    "    \n",
    "- If some instances are clearly **outliers**, it may help to simply discard them or try to fix the errors manually.\n",
    "- If some instances are **missing a few features** (e.g., 5% of your customers did not specify their age), you must decide whether you want to:\n",
    "    - Ignore this attribute altogether\n",
    "    - Ignore only these instances\n",
    "    - Fill in the missing values (e.g., with the median age). \n",
    "    - Or train one model with the feature and one model without it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00d88d-60e6-4e20-95a2-7631368f59bc",
   "metadata": {},
   "source": [
    "### 2.1.4 Irrelevant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f340a-1b27-47c9-9b06-adad1c5cfdde",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "As the saying goes: garbage in, garbage out. Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a machine learning project is to coming up with a good set of features to train on. This process is called ***feature engineering***. It involves the following steps:\n",
    "\n",
    "1- ***Feature selection***: Selecting the most useful features to train on amoing existing features.<br>\n",
    "2- ***Feature Extraction***: Combining existing features to produce a more useful one: *dimentionality reeduction can help here*.\n",
    "3- ***Creating New features***: Creating new features by gathering new data. \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b48fbe0",
   "metadata": {},
   "source": [
    "### 2.1.5 How Adding more Features could Enhance the Model's Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57186805",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "Imagine you have a set of points on a 2D plane (x, y coordinates), representing two classes of data (say, red and blue). If these points are not linearly separable on this plane, it is difficult to find a straight line that can separate the red points from the blue points.\n",
    "\n",
    "Now, consider that you have access to a third dimension (z coordinate) for each point. By plotting the data points in a 3D space (x, y, z coordinates), you may be able to find a plane that separates the red points from the blue points more effectively than in the 2D space. This is because the additional dimension provides more flexibility in finding a decision boundary that can separate the data points belonging to different classes.\n",
    "\n",
    "Adding more dimensions (attributes) can enhance the performance of a machine learning model by providing it with more information to learn from, allowing it to better capture the underlying patterns and relationships in the data. In turn, this can lead to improved performance in terms of accuracy, precision, recall, or other relevant metrics.\n",
    "\n",
    "However, adding more dimensions can also have potential downsides, such as overfitting and increased computational cost. Therefore, it is crucial to carefully select the most relevant and informative features for your model and apply feature engineering techniques to make the most of the additional information while minimizing the risks associated with increased complexity.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Imagine you're trying to predict the price of a house based on its features. You start with a simple model using only two features: square footage and the number of bedrooms. In this case, you can visualize the relationship between these features and the house price in a 3D space (2D for features, 1D for the target variable - price).\n",
    "\n",
    "Now, consider that you want to improve the model's performance by adding more features, such as the number of bathrooms, the age of the house, proximity to public transportation, the quality of local schools, and so on. As you add more features, the number of dimensions in your feature space increases.\n",
    "\n",
    "With each additional dimension, the volume of the feature space grows exponentially. For instance, if you have a dataset with 1,000 data points and two features, these points might be relatively evenly distributed across the 2D feature space. However, when you increase the number of features to, say, 10, the same 1,000 data points are now spread across a much larger, 10-dimensional space.\n",
    "\n",
    "As a consequence, the data becomes sparser in this higher-dimensional space, meaning that the average distance between data points increases, and the model has fewer examples to learn from in each region of the space. This can lead to overfitting, as the model is more likely to memorize the training data instead of learning the underlying patterns that would help it generalize to new, unseen data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e67b34-34de-4787-8317-b44b36bf4143",
   "metadata": {},
   "source": [
    "## 2.2. Bad Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a5b86-bf73-427a-b776-43036631bfe7",
   "metadata": {},
   "source": [
    "### 2.2.1 Overfitting the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343bc1c-e852-4615-ab8f-61ba5f3918b3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "***Overfitting***: when the model performs well on the training data, but does not generalize well. E.g., a high degree polynomial may overfit a simple linear looking data. Complex models such as deep neural networks can detect subtle patterns in the data, but if the training data is noisy, or if it is too small (which introduces ***sampling noise***), then the model will likely to detect patterns in the noise itself. Obviously these patterns will not generalize to new instances. \n",
    "    \n",
    "For instance, suppose we fed a model with countries' names as attributes, a complex model may detect patterns like the fact that all countries in the training data with a $w$ in their name have life satisfaction greater than 7: New Zealand (7.3), Norway (7.4), Sweden (7.2), and switzerland (7.5). However, how confident are you that the $w$-satisfaction rule generalizes to Rwanda or Zimbabwe?\n",
    "    \n",
    "Therefore, some pattern occur in the training data by pure chance, but the model has no way to tell whether a pattern is real or simply the result of noise in the data.\n",
    "    \n",
    "*Definition:** Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c018b18-c1ee-4ea6-bbdc-bab77a2f59ea",
   "metadata": {},
   "source": [
    "#### Possible Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae46da1-20ec-42d0-a00a-dcf1648df331",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "- **Simplify the model:** by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model). Constraining the model to make it simpler and reduce the risk of overfitting is called *regularization*. For example, if we have a linear model with two parameters: $\\theta_0$: intercept, and $\\theta_1$: slope. This gives the learning algorithm two degrees of freedom to adapt the model to the training data: it can tweak both the height $(\\theta_0)$, and the slope $(\\theta_1)$ of the line. If we force $\\theta_1 = 0$, the algorithm would have only one degree o freedom and would have a much harder time fitting the data properly: all it could do is move the line up and down to get as close as possible to the training instances, so it would end up around the mean. A very simple model indeed! If we allow the algorithm to modify $\\theta_1$ but we force it to keep it small, then the learning algorithm will effectively have somewhere in between one and two degrees of freedom. Hence, it will produce a model that is simpler than the one with two degrees of freedom, but more complex than the one with 1 degree of freedom. <br>**You want to find the right balance between fitting the training data perfectly and keeping the model simple enough to ensure that it will generalize well.** <br>The amount of regularization to apply during learning/training can be controlled by a *hyperparameter*. A Hyperparameter is a parameter of the learning algorithm (not of the model). A hyperparameter is maybe added to the loss function, hence, is not part of the model. As such, it is not affected by the learning algorithm itself, aka, does not change while training the model, but must be set prior to training and remains constant during training.   \n",
    "- **Reduce the number of attributes** in the training data. E.g., sometimes onehot encoding could turn out to degrade the performance of a model. And we refer to this as the curse of dimensionality (more on this later).\n",
    "- **Gather more data**.\n",
    "- **Reduce the noise** in the training data (e.g., fix data errors and remove outliers).\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f51e1acc",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bff2f22",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "Gathering more data can help reduce overfitting because it provides a more diverse and representative sample of the true underlying patterns in the data. A larger dataset can make it harder for the model to memorize specific instances or learn irrelevant patterns present in a smaller dataset.\n",
    "\n",
    "Let's consider a real-world example with an e-commerce website where you want to build a model to predict whether a customer will make a purchase based on their browsing behavior.\n",
    "\n",
    "Suppose you have a small dataset of 100 customers. In this small dataset, you might find that 90% of customers who visit a specific product page (e.g., Product A) make a purchase. Your model could learn that visiting Product A's page is a strong indicator of making a purchase, resulting in a decision boundary that overemphasizes this feature.\n",
    "\n",
    "Now imagine you collect more data, expanding your dataset to 10,000 customers. With this larger dataset, you find that only 10% of customers who visit Product A's page actually make a purchase. The new data helps the model understand that visiting Product A's page is not as strong an indicator of making a purchase as it initially seemed. Consequently, the model learns a more nuanced decision boundary that is less likely to overfit.\n",
    "\n",
    "In this example, the larger dataset provided a more accurate representation of the true relationship between the features (browsing behavior) and the target variable (making a purchase). The model trained on more data is less likely to overfit because it learned a more generalized pattern instead of relying on specific instances or features that were overemphasized in the smaller dataset.\n",
    "\n",
    "However, it is important to note that collecting more data alone might not always guarantee better generalization. The quality of the data, the presence of noise, and the relevance of the features also play crucial roles in determining the model's performance.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a290bb-b3d2-4abd-a5e2-d0087a45faa5",
   "metadata": {},
   "source": [
    "### 2.2.2 Underfitting the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672d6d4-78e5-4abb-a028-8f73fff93c84",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "This is the opposite of overfitting. It occurs when your model is too simple to learn the underlying structure of the data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5c25f-2834-44b7-9232-3a7097c883f9",
   "metadata": {},
   "source": [
    "#### Possible Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22689ed4-9094-42c1-9508-3d92f67d6bbe",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "- Select a more powerful model, with **more parameters**.\n",
    "- Feed **better features** to the learning algorithm (feature engineering).\n",
    "- **Reduce the contraints** on the model (e.g., reduce the regularization hyperparameter).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e995b-8990-4f16-be53-05c1604066a3",
   "metadata": {},
   "source": [
    "# 3. Testing and Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2668571-ef64-4098-b4f2-e9df9a3dc6d3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "A good idea is to split your data into two sets: the *training set* and the *test set*. As these names imply, you train your model using the training set, and you test it using the testing set. The error rate on the new cases is called the *generalization error* (or *out-of-sample* error). And by evaluating your model on the test set, you get an estimate of this error. This value tells you how well your model will perform on instances it has never seen before.\n",
    "    \n",
    "If the training error is low (i.e., your model makes few mistakes on the training set but the generalization error is high, it means that the model is overfitting the training set).\n",
    "    \n",
    "**Note:** It is common to use 80% of the data for training and hold our 20% for testing. However, this depends on the size of the dataset. if it contains 10 million instances, then holding out 1% means your test set will contain 100,000 instance, probably mmore than enough to get a good estimate of the generalization error. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df471d-7c05-4bec-b3f6-a5bce9d666b7",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter Tuning and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b432620-c09b-4db4-993d-f60f0a085dad",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "Evaluating a model is simple enough. Just use a test set. But what if we are hesitant between 2 models (say a linear and a polynomial model): how to decide between them? One option is to train both and compare how well they generalize using the test set.\n",
    "\n",
    "Now, maybe after testing, we found out that the linear model generalizes better, but you want to apply some regularization to avoid overfitting. The question is, how do you choose the value of the regularization hyperparmater. One option is to train 100 different models using 100 different values of this hyperparameter. Suppose you find the best hyperparameter value that produces a model with the lowest generalization error - say, 5% error. Finally, after putting the model into production, we get an error of 15% on unseen data. What just happened?\n",
    "\n",
    "The problem is that you measured the generalization error multiple times on the test set, and you adapted the model and hyperparameters to produce the best model for that particular set. This means that the model is unlikely to perform as well on new data.\n",
    "\n",
    "A common solution to this problem is called *holdout validation*: You simply hold out part of the training set to evaluate several candidate models and select the best one. The new held-out set is called *validation set* (or sometimes the *development set*, or *dev set*). More specifically, you train multiple models with various hyperparameters on the reduced training set (train set - dev set), and then select the model that performs best on the validation set. After this holdout validation process, you train the model again on the full training set (including the dev set) and this will gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generlization error.\n",
    "    \n",
    "This solution usually works quite well. However, if the validation set is too small, then model evaluations will be imprecise: you may end up selecting a suboptimal model by mistake. Conversely, if the validation set is too large, then the remaining training set will be much smaller than the full training set. Why this is bad? Well, since the final model will be trained on the full training set, it is not ideal to compare candidate models trained on a much smaller training set. It would be like selecting the fastest sprinter to participate in a marathon. One way to solve this problem is to perform repeated *cross-validation*, using many small validation sets. Each model is evaluated once per validation set, after it is trained on the rest of the data. By averaging out all the evaluations of a model, we get a much more accurate measure of its performance. However, there is a drawback: the training time is multiplied by the number of validation sets.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191d9ac-4cab-4aab-95d0-4ff007f36082",
   "metadata": {},
   "source": [
    "# 5. Data Mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1602d-74b5-4f3e-8077-63a970630215",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "In some cases, it is easy to get a large amount of data for training, but it is not perfectly representative of the data that will be used in production. For example, suppose you want to create a mobile app to take pictures of flowers and automatically determine their species. You can easily download millions of pictures of flowers on the web, but they won’t be perfectly representative of the pictures that will actually be taken using the app on a mobile device. Perhaps you only have 10,000 representative pictures (i.e., actually taken with the app). In this case, the most important rule to remember is that the validation set and the test must be as representative as possible of the data you expect to use in production, so they should be composed exclusively of representative pictures: you can shuffle them and put half in the validation set, and half in the test set (making sure that no duplicates or near-duplicates end up in both sets). After training your model on the web pictures, if you observe that the performance of your model on the validation set is disappointing, you will not know whether this is because your model has overfit the training set, or whether this is just due to the mismatch between the web pictures and the mobile app pictures. One solution is to hold out part of the training pictures (from the web) in yet another set that Andrew Ng calls the train-dev set. After the model is trained (on the training set, not on the train-dev set), you can evaluate it on the train-dev set: if it performs well, then the model is not overfitting the training set, so if performs poorly on the validation set, the problem must come from the data mismatch. You can try to tackle this problem by preprocessing the web images to make them look more like the pictures that will be taken by the mobile app, and then retraining the model. Conversely, if the model performs poorly on the train-dev set, then the model must have overfit the training set, so you should try to simplify or regularize the model, get more training data and clean up the training data, as discussed earlier.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0ab2ce6",
   "metadata": {},
   "source": [
    "# 6. Affect of learning rate on the training algorithm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87c9e78f",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "One important parameter of online learning systems is how fast they should adapt to changing data: this is called the learning rate. If you set a high learning rate, then your system will rapidly adapt to new data, but it will also tend to quickly forget the old data (you don’t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it will learn more slowly, but it will also be less sensitive to noise in the new data or to sequences of nonrepresentative data points (outliers).\n",
    "\n",
    "In the presence of changing data distribution, the loss function's shape will also change. This is because the optimal model parameters for the old data distribution may not be optimal for the new data distribution.\n",
    "\n",
    "Let's break down the learning rate's role in this scenario:\n",
    "\n",
    "1. **High learning rate**: When the learning rate is high, the model quickly adapts to the new data distribution. It takes larger steps in the parameter space to minimize the loss function. This is beneficial when the data distribution is genuinely changing, as the model can swiftly adapt to the new patterns. However, a high learning rate can also make the model sensitive to noise or fluctuations in the data. It might lead to overshooting the optimal parameters, causing the model to oscillate around the minimum of the loss function and potentially diverging.\n",
    "   \n",
    "2. **Low learning rate**: With a low learning rate, the model takes smaller steps in the parameter space, which means it learns more slowly. This can be advantageous when the data has noise or fluctuations, as the model is less likely to be influenced by non-representative data points. However, if the data distribution changes significantly, a low learning rate may not allow the model to adapt quickly enough, leading to poor performance on the new data.\n",
    "\n",
    "### How outliers affect the loss function:\n",
    "\n",
    "As for the loss curve, the loss function's value over time, In the presence of changing data distribution, the loss curve will also exhibit changes. When the data distribution shifts, the model may experience an increase in the loss value as it tries to adapt to the new distribution. The learning rate's role will then influence how quickly the model can minimize the new loss function and how stable the loss curve is throughout the training process.\n",
    "\n",
    "The model complexity plays a role in how it reacts to anomalies. If the model is very complex, it might be more prone to overfitting to the anomalies, even when it's close to the global minimum. A simpler model may be more robust to the presence of anomalies.\n",
    "\n",
    "When outliers are present in the data, the MSE loss function becomes more sensitive to these points. This is because the squared error term grows quadratically with the magnitude of the error, causing outliers to have a disproportionately large impact on the overall loss value.\n",
    "\n",
    "In the presence of outliers, the bowl-shaped loss surface can become distorted. The global minimum of the loss function might shift towards the outliers, as the model tries to minimize the overall error by fitting these points more closely. As a result, the model parameters may be biased towards the outliers, leading to a less accurate representation of the underlying data distribution.\n",
    "\n",
    "In summary, the learning rate plays a crucial role in determining how a model adapts to changing data distributions. A high learning rate allows for faster adaptation but can lead to instability, while a low learning rate provides more stability but may not adapt quickly enough to significant changes. The loss curve will also be affected by the changing data distribution and the learning rate, reflecting the model's ability to optimize the loss function under varying conditions.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c6e5430",
   "metadata": {},
   "source": [
    "## 6.1. The effect of high learning rate with the presence of anomalies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fded1ede",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "In the presence of anomalies or outliers in the data, a high learning rate can lead to several challenges for a machine learning model. Here's what can happen:<br>\n",
    "\n",
    "1. **Oversensitivity to noise**: A high learning rate means that the model will take larger steps in the parameter space during optimization. Consequently, the model will be more sensitive to noise or anomalies in the data. When an anomaly is encountered, the model may treat it as a significant change in the data distribution and adjust its parameters accordingly. This can lead to the model learning incorrect patterns or fitting the noise instead of the underlying true patterns in the data.\n",
    "\n",
    "2. **Loss function oscillation**: When the learning rate is high, the model may overshoot the optimal parameters while trying to minimize the loss function. This can cause the model to oscillate around the minimum of the loss function, making it harder to converge. In the presence of anomalies, this oscillation can be exacerbated, as the model keeps adjusting its parameters back and forth in response to the noise.\n",
    "\n",
    "3. **Difficulty in convergence**: Due to the larger steps taken in the parameter space, a high learning rate can make it challenging for the model to converge to the optimal parameters, especially when there are anomalies in the data. The model may keep bouncing between different regions of the parameter space, influenced by the noise or outliers, leading to poor convergence or even divergence.\n",
    "\n",
    "4. **Reduced generalization**: When the model is influenced by anomalies or noise, it may overfit to the training data, capturing patterns specific to the anomalies rather than the overall data distribution. This can reduce the model's ability to generalize well to new, unseen data, leading to lower performance on validation or test datasets.\n",
    "\n",
    "In summary, a high learning rate can cause several issues when there are anomalies in the data, such as oversensitivity to noise, oscillations in the loss function, difficulty in convergence, and reduced generalization. It is essential to choose an appropriate learning rate that balances the model's ability to adapt to new data while maintaining stability and minimizing the influence of noise or anomalies. In many cases, using techniques like learning rate scheduling or adaptive learning rates can help mitigate the effects of a high learning rate and improve model performance.\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5354077-0c48-4e6c-83bf-c24da407b836",
   "metadata": {},
   "source": [
    "# 7. No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3547d9-88bd-4fac-aaa4-7363800662d1",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14pt; color:black; font-family:Garamond\">\n",
    "\n",
    "A model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. However, to decide what data to discard and what data to keep, you must make assumptions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored.\n",
    "\n",
    "In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f42d4-c8a2-43f5-bf69-4a38983a0390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
